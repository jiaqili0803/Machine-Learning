{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWTe9VS3_b11"
   },
   "source": [
    "## SI 670 Applied Machine Learning, Week 9: dimension reduction, deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4h5cjV83rlY"
   },
   "source": [
    "## Problem 1 (30 points)\n",
    "\n",
    "For this problem, you will be comparing PCA (Principal Component Analysis) and MDS (Multi-Dimensional Scaling). We will be using our ever-familiar breast cancer dataset. We will also use `GradientBoostingClassifier` with `learning_rate= 0.1` and `n_estimators=100` (i.e. the default values) as our classifier.\n",
    "\n",
    "### For PCA:\n",
    "In the model we want to use for PCA, we use `PCA` to transform our data and then use `GradientBoostingClassifier` to classify the data. What we need to figure out is the best value of `n_components` for `PCA`. To do so, we will apply cross-validation using `GridSearchCV` on a `Pipeline`. Please consider `n_components = 2, 3, 5` for the parameter grid, and please report the test score of the best model.\n",
    "\n",
    "### For MDS:\n",
    "Notice that `MDS` is a dimensionality reduction tool that preserves pairwise distance. Unlike PCA which learns a linear transform and can apply it to future data, MDS can only learn embeddings for the specified data. Which is why `MDS` and does not have a stand-alone `transform()` method, and can only be applied to data that was fit to it. As a consequence, we are not able to learn MDS transforms on some data and then apply them to some other data. \n",
    "\n",
    "Therefore, you should first apply `fit_transform()` from `MDS` to ALL data. Then split it into training and test, fit `GradientBoostingClassifier` on training data, and then compute score on test data. You should repeat this process for three possible values of the `n_components` parameter of `MDS`: `n_components = 2, 3, 5`. Once you have done so, please report the best score on test data.\n",
    "\n",
    "(Please note that you do not need to scale the data for either of these two parts.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kiHBepck7ETY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BEST PCA test score is: 0.9300699300699301\n",
      "[0.9370629370629371, 0.9300699300699301, 0.916083916083916]\n",
      "The BEST MDS test score is: 0.9370629370629371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9300699300699301, 0.9370629370629371)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def answer_one():\n",
    "    cancer = load_breast_cancer()\n",
    "    (X_cancer, y_cancer) = load_breast_cancer(return_X_y = True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state = 0)\n",
    "\n",
    "    ######## Your code for PCA here\n",
    "    # parameter\n",
    "    param_grid = {\"pca__n_components\": [2, 3, 5]}\n",
    "    # build pipeline\n",
    "    components = [('pca', PCA()), ('clf', GradientBoostingClassifier())]\n",
    "    pipe = Pipeline(components)\n",
    "    # grid search a pipeline\n",
    "    grid = GridSearchCV(pipe, param_grid)\n",
    "    # GridSearchCV itself can be viewed as a classifier, can implement .fit and .score functions\n",
    "    # tune with GridSearchCV\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model_test_score_PCA = grid.score(X_test, y_test)\n",
    "    # best parameters\n",
    "    pca_best_n_components = grid.best_params_['pca__n_components']\n",
    "    # print('The best PCA parameters (i.e. best model) for n_components:', pca_best_n_components)\n",
    "    print('The BEST PCA test score is:', best_model_test_score_PCA)\n",
    "\n",
    "    ######## Your code for MDS here\n",
    "    # we cannot really use MDS on training/validation/test data separately, \n",
    "    # since that would affect the resulting pairwise distances between points that are on different datasets.\n",
    "    # so, just in this question, apply MDS to All data (but this do not work for other real world examples)\n",
    "    \n",
    "    (X, y) = load_breast_cancer(return_X_y = True)\n",
    "    \n",
    "    n_components_list = [2, 3, 5]\n",
    "    score_list = []\n",
    "    for i in n_components_list:\n",
    "        \n",
    "        # apply MDS on all X data\n",
    "        mds = MDS(n_components = i)\n",
    "        X = mds.fit_transform(X)\n",
    "        \n",
    "        # split train-test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 0)\n",
    "        \n",
    "        # fit GradientBoostingClassifier on training data\n",
    "        clf = GradientBoostingClassifier()\n",
    "        clf.fit(X_train, y_train)\n",
    "        # compute score on test data\n",
    "        this_score = clf.score(X_test, y_test)\n",
    "        score_list.append(this_score)\n",
    "        \n",
    "    # report max score\n",
    "    print(score_list)\n",
    "    best_model_test_score_MDS = np.max(score_list)\n",
    "    print('The BEST MDS test score is:', best_model_test_score_MDS)\n",
    "  \n",
    "    return best_model_test_score_PCA, best_model_test_score_MDS\n",
    "\n",
    "answer_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjJ3ARjZ7j4x"
   },
   "source": [
    "## Problem 2 (30 points)\n",
    "\n",
    "### (a)\n",
    "Leon and Claire has access to some data about viral infection in Raccoon City, where the target variable is the number of people who will get infected in a given day. What type of activation should they use on the final layer?\n",
    "\n",
    "### (b)\n",
    "Jill is training a multi-layer perceptron. While the error on the training data is low, the error on the test data is high. In order to improve generalization to test data Jill increases the number of layers in the model. Is this likely to work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HJStYRB96RA"
   },
   "source": [
    "#### Your answer for 2(a) here:\n",
    "* the number of people who will get infected -- they are trying to do Regression: Predicting a numerical value, so the Final Activation Function should be Linear (This results in a numerical value which we require) or ReLU (This results in a numerical value greater than 0).\n",
    "\n",
    "#### Your answer for 2(b) here:\n",
    "* No, Jill's method will not work. The error on the training data is low, the error on the test data is high -- this MLP is overfitting -- the basic idea to deal with the problem of overfitting is to decrease the complexity of the model. To do so, we can make the network smaller by simply removing the layers or reducing the number of neurons, etc.\n",
    "\n",
    "* ref:\n",
    "*https://towardsdatascience.com/deep-learning-which-loss-and-activation-functions-should-i-use-ac02f1c56aa8\n",
    "*https://www.analyticsvidhya.com/blog/2021/06/complete-guide-to-prevent-overfitting-in-neural-networks-part-1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQgT5isO-B8A"
   },
   "source": [
    "## Problem 3 (40 points)\n",
    "\n",
    "You will be building a multi-layer perceptron using `keras` for classification of the breast cancer dataset.\n",
    "\n",
    "Your perceptron should have three hidden layers, each with 32 neurons. The hidden layers should use `tanh` activation.\n",
    "\n",
    "Since we are doing a classification problem with two classes, your output layer needs to have two neurons. You should use `sigmoid` activation for the output layer.\n",
    "\n",
    "Please report the accuracy score of your model on the test data for 1000 epochs and batch size 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tensorflow --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-vl8KrB-_ep3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.2791 - accuracy: 0.9510\n",
      "0.9510489702224731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9510489702224731"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def answer_three():\n",
    "    (X_cancer, y_cancer) = load_breast_cancer(return_X_y = True)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state=0)\n",
    "    # print(y_test)\n",
    "\n",
    "    # Scale your data using standard scaler. Beware of leakage!\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Your code for the perceptron here\n",
    "    network = Sequential()\n",
    "    \n",
    "    # hidden layers\n",
    "    network.add(Dense(32, activation = 'tanh', input_shape = X_train_scaled[0].shape))\n",
    "    network.add(Dense(32, activation = 'tanh'))\n",
    "    network.add(Dense(32, activation = 'tanh'))\n",
    "    \n",
    "    # classification problem with two classes -- output layer needs to have two neurons\n",
    "    # output layer\n",
    "    network.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    ################## rashape label y to (,2) ################\n",
    "    # print(\"before label reshape:\", y_test)\n",
    "    y_train = np.asarray(y_train).astype('float32').reshape((-1,1))\n",
    "    y_test = np.asarray(y_test).astype('float32').reshape((-1,1))\n",
    "    # print(\"after label reshape:\", y_test)\n",
    "    ##################################\n",
    "    \n",
    "    \n",
    "    # compile \n",
    "    network.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "    network.fit(X_train_scaled, y_train, epochs = 1000, batch_size = 200, verbose = False)\n",
    "    # \n",
    "    _, test_accuracy = network.evaluate(X_test_scaled, y_test)\n",
    "    print(test_accuracy)\n",
    "\n",
    "    return test_accuracy\n",
    "\n",
    "answer_three()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
